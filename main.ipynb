{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main\n",
    "Michael de Jong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nbconvert\n",
    "import nbformat\n",
    "import zipfile\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load local python file(s)\n",
    "%run evaluationmetrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "0.75\n",
      "0.4285714285714285\n",
      "0.262359022556391\n"
     ]
    }
   ],
   "source": [
    "# 1 = relevant, 0 = not relevant\n",
    "l = [1,1,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,1]\n",
    "c = 8\n",
    "print(precision(l))\n",
    "print(recall(l,c))\n",
    "print(f1(l,c))\n",
    "print(avg_precision(l,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to local elastic search host\n",
    "HOST = 'http://localhost:9200/'\n",
    "es = Elasticsearch(hosts=[HOST])\n",
    "\n",
    "# global vars\n",
    "INDEX=\"notebookindex\"\n",
    "TYPE= \"notebook\"\n",
    "COLUMNS = ['nb_id', 'html_url', 'name', 'language', 'markdown', 'comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_id</th>\n",
       "      <th>html_url</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/dalequark/emotivExperiment/...</td>\n",
       "      <td>EmotivDataAnalysis.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/kevcisme/madelon_redux/blob...</td>\n",
       "      <td>Part_IV_Project_3-checkpoint_BASE_63907.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://github.com/HaraldoFilho/DLND-Projects/...</td>\n",
       "      <td>_.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://github.com/mhjensen/CPMLS/blob/4a5b37e...</td>\n",
       "      <td>csexmas2015.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://github.com/freqn/atom_configuration/bl...</td>\n",
       "      <td>jupyter.ipynb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nb_id                                           html_url  \\\n",
       "0      0  https://github.com/dalequark/emotivExperiment/...   \n",
       "1      1  https://github.com/kevcisme/madelon_redux/blob...   \n",
       "2      2  https://github.com/HaraldoFilho/DLND-Projects/...   \n",
       "3      3  https://github.com/mhjensen/CPMLS/blob/4a5b37e...   \n",
       "4      4  https://github.com/freqn/atom_configuration/bl...   \n",
       "\n",
       "                                            name  \n",
       "0                       EmotivDataAnalysis.ipynb  \n",
       "1  Part_IV_Project_3-checkpoint_BASE_63907.ipynb  \n",
       "2                                        _.ipynb  \n",
       "3                              csexmas2015.ipynb  \n",
       "4                                  jupyter.ipynb  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the csv with all notebook information\n",
    "df_nb = pd.read_csv('notebooks.csv')\n",
    "df_nb = df_nb.drop(['max_filesize','min_filesize', 'path', 'query_page', 'repo_id'], axis=1)\n",
    "#print('%s notebooks' % df_nb.shape[0])\n",
    "df_nb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile('bb2733859v_2_1.zip', 'r') as nbzip:\n",
    "    with nbzip.open('nb_1.ipynb') as test:\n",
    "        print(test.read())\n",
    "test = archive1.read('nb_1.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198778\n",
      "198778\n"
     ]
    }
   ],
   "source": [
    "folder = 'bb2733859v_2_1'\n",
    "filenames = os.listdir(folder)\n",
    "print(len(filenames))\n",
    "def get_ids(path):\n",
    "    filenames = os.listdir(path)\n",
    "    ids = []\n",
    "    for i in range(len(filenames)):\n",
    "        if filenames[i].startswith('nb_') and filenames[i].endswith('.ipynb'):\n",
    "            current_id = int(filenames[i][3:-6])\n",
    "            ids.append(current_id)\n",
    "        else:\n",
    "            print('miss')\n",
    "    return ids\n",
    "\n",
    "ids = get_ids(folder)\n",
    "print(len(ids)) # check same length\n",
    "# row = df_nb.loc[df_nb['nb_id'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, None)\n"
     ]
    }
   ],
   "source": [
    "# extract markdown, language and comments\n",
    "# path = \"./sample_data/data/notebooks/\"\n",
    "# notebook_loc = path + 'nb_1222.ipynb'\n",
    "def get_text(path):\n",
    "    markdown = []\n",
    "    comments = []\n",
    "    with open(path) as fp:\n",
    "        try:\n",
    "            data = nbformat.read(fp, nbformat.NO_CONVERT)\n",
    "            #data = json.load(fp)\n",
    "        except:\n",
    "            return None, None, None\n",
    "    if 'cells' in data:\n",
    "        cells = data['cells']\n",
    "        if 'metadata' in data:\n",
    "            if 'kernelspec' in data['metadata']:\n",
    "                if 'language' in data['metadata']['kernelspec']:\n",
    "                    language = data['metadata']['kernelspec']['language']\n",
    "                else:\n",
    "                    language = None\n",
    "            else:\n",
    "                language = None\n",
    "        else:\n",
    "            language = None\n",
    "        md_cells = [c for c in cells if c['cell_type'] == 'markdown']\n",
    "        code_cells = [c for c in cells if c['cell_type'] == 'code']\n",
    "        for cell in md_cells:\n",
    "            markdown.append(cell['source'])\n",
    "\n",
    "        # find comments '# ' for R and Python\n",
    "        for cell in code_cells:\n",
    "            source = cell['source']\n",
    "            string = ''\n",
    "            if source != None:\n",
    "                for item in source:\n",
    "                    string = string + str(item)\n",
    "                if '# ' in string:\n",
    "                    comments.append(string)\n",
    "        return language, markdown, comments\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "# l,m,c = get_text(notebook_loc)\n",
    "#print(c)\n",
    "print(get_text('bb2733859v_2_1/nb_0.ipynb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "O:\\Anaconda3\\lib\\site-packages\\nbformat\\validator.py:251: UserWarning: No schema for validating v2 notebooks\n",
      "  warnings.warn(\"No schema for validating v%s notebooks\" % version, UserWarning)\n",
      "O:\\Anaconda3\\lib\\site-packages\\nbformat\\validator.py:251: UserWarning: No schema for validating v1 notebooks\n",
      "  warnings.warn(\"No schema for validating v%s notebooks\" % version, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# append all extra columns to dataframe and export to csv\n",
    "final_df = pd.DataFrame(columns = COLUMNS)\n",
    "for i in range(len(ids)):\n",
    "    path = folder + '/nb_' + str(ids[i]) + '.ipynb'\n",
    "    language, markdown, comments = get_text(path)\n",
    "    if language != None or markdown != None or comments != None:\n",
    "        row = df_nb.loc[df_nb['nb_id'] == ids[i]]\n",
    "        final_df = final_df.append({COLUMNS[0]:row['nb_id'].values[0],\n",
    "                                    COLUMNS[1]:row['html_url'].values[0],\n",
    "                                    COLUMNS[2]:row['name'].values[0],\n",
    "                                    COLUMNS[3]:language,\n",
    "                                    COLUMNS[4]:markdown,\n",
    "                                    COLUMNS[5]:comments},\n",
    "                                   ignore_index=True)\n",
    "# save the dataframe to csv\n",
    "final_df.to_csv('df_bb2733859v_2_1.csv')\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_df = pd.read_csv('df_bb2733859v_2_1.csv')\n",
    "\n",
    "bulk_data = []\n",
    "\n",
    "for index, row in es_df.iterrows():\n",
    "    data_dict = {}\n",
    "    for i in range(len(row)):\n",
    "        data_dict[es_df.columns[i]] = row[i]\n",
    "    op_dict = {\n",
    "        \"index\": {\n",
    "            \"_index\": INDEX,\n",
    "            \"_type\": TYPE,\n",
    "            \"_id\": data_dict['nb_id']\n",
    "        }\n",
    "    }\n",
    "    bulk_data.append(op_dict)\n",
    "    bulk_data.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the elastic search index\n",
    "init_index = {\n",
    "    \"settings\" : {\n",
    "        \"number_of_shards\": 5,\n",
    "        \"number_of_replicas\": 1\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"text_analyzer\": {\"type\": \"standard\"}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'mappings': {\n",
    "        'notebook': {\n",
    "            'properties': {\n",
    "                'nb_id': {'index': 'not_analyzed', 'type': 'string'},\n",
    "                'html_url': {'index': 'not_analyzed', 'type': 'string'},\n",
    "                'name': {'index': 'not_analyzed', 'type': 'string'},\n",
    "                'language': {'index': 'not_analyzed', 'type': 'string'},\n",
    "                'markdown': {'index': 'analyzed', 'type': 'string'},\n",
    "                'comments': {'index': 'analyzed', 'type': 'string'},\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.indices.create(index = INDEX, body = init_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = es.bulk(index = INDEX, body = bulk_data)\n",
    "\n",
    "# check if the data and structure is in elasticsearch\n",
    "es.search(body={\"query\": {\"match_all\": {}}}, index = INDEX)\n",
    "es_indices.get_mapping(index = INDEX)\n",
    "\n",
    "# datadict = df_nb.todict()\n",
    "# res = es.index(index=INDEX, doc_type='notebook', body=datadict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
