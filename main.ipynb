{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main\n",
    "Michael de Jong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nbconvert\n",
    "import nbformat\n",
    "import zipfile\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load local python file(s)\n",
    "%run evaluationmetrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "0.75\n",
      "0.4285714285714285\n",
      "0.262359022556391\n"
     ]
    }
   ],
   "source": [
    "# 1 = relevant, 0 = not relevant\n",
    "l = [1,1,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,1]\n",
    "c = 8\n",
    "print(precision(l))\n",
    "print(recall(l,c))\n",
    "print(f1(l,c))\n",
    "print(avg_precision(l,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to local elastic search host\n",
    "HOST = 'http://localhost:9200/'\n",
    "es = Elasticsearch(hosts=[HOST])\n",
    "\n",
    "# global vars\n",
    "INDEX=\"notebookindex\"\n",
    "TYPE= \"notebook\"\n",
    "COLUMNS = ['nb_id', 'html_url', 'name', 'language', 'markdown', 'comments', 'code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_id</th>\n",
       "      <th>html_url</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/dalequark/emotivExperiment/...</td>\n",
       "      <td>EmotivDataAnalysis.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/kevcisme/madelon_redux/blob...</td>\n",
       "      <td>Part_IV_Project_3-checkpoint_BASE_63907.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://github.com/HaraldoFilho/DLND-Projects/...</td>\n",
       "      <td>_.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://github.com/mhjensen/CPMLS/blob/4a5b37e...</td>\n",
       "      <td>csexmas2015.ipynb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://github.com/freqn/atom_configuration/bl...</td>\n",
       "      <td>jupyter.ipynb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nb_id                                           html_url  \\\n",
       "0      0  https://github.com/dalequark/emotivExperiment/...   \n",
       "1      1  https://github.com/kevcisme/madelon_redux/blob...   \n",
       "2      2  https://github.com/HaraldoFilho/DLND-Projects/...   \n",
       "3      3  https://github.com/mhjensen/CPMLS/blob/4a5b37e...   \n",
       "4      4  https://github.com/freqn/atom_configuration/bl...   \n",
       "\n",
       "                                            name  \n",
       "0                       EmotivDataAnalysis.ipynb  \n",
       "1  Part_IV_Project_3-checkpoint_BASE_63907.ipynb  \n",
       "2                                        _.ipynb  \n",
       "3                              csexmas2015.ipynb  \n",
       "4                                  jupyter.ipynb  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the csv with all notebook information\n",
    "df_nb = pd.read_csv('notebooks.csv')\n",
    "df_nb = df_nb.drop(['max_filesize','min_filesize', 'path', 'query_page', 'repo_id'], axis=1)\n",
    "#print('%s notebooks' % df_nb.shape[0])\n",
    "df_nb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'bb2733859v_2_1'\n",
    "# filenames = os.listdir(folder)\n",
    "# print(len(filenames))\n",
    "def get_ids(path):\n",
    "    filenames = os.listdir(path)\n",
    "    ids = []\n",
    "    for i in range(len(filenames)):\n",
    "        if filenames[i].startswith('nb_') and filenames[i].endswith('.ipynb'):\n",
    "            current_id = int(filenames[i][3:-6])\n",
    "            ids.append(current_id)\n",
    "        else:\n",
    "            print('miss')\n",
    "    return ids\n",
    "\n",
    "def get_ids_zip(path):\n",
    "    path = path + '.zip'\n",
    "    nbzip = zipfile.ZipFile(path, 'r')\n",
    "    filenames = nbzip.namelist()\n",
    "    ids = []\n",
    "    for i in range(len(filenames)):\n",
    "        if filenames[i].startswith('nb_') and filenames[i].endswith('.ipynb'):\n",
    "            current_id = int(filenames[i][3:-6])\n",
    "            ids.append(current_id)\n",
    "        else:\n",
    "            print('miss')\n",
    "    return ids, nbzip\n",
    "\n",
    "#ids = get_ids(folder)\n",
    "ids, nbzip = get_ids_zip(folder)\n",
    "# print(len(ids)) # check same length\n",
    "# row = df_nb.loc[df_nb['nb_id'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('python', ['### Therefore Training accuracy of 93.50% and Validation accuracy of 96.6% achieved in 200 epochs of a neural network with 1 hidden layer comprising of 4 neurons.'], [\"# Keras model\\n\\nmodel = Sequential()\\n\\nfc1 = Dense(4, input_shape=(2,))           #This is the first hidden layer, therefore sigmoid function as activation and 4 neurons\\nmodel.add(fc1)\\nmodel.add(Activation('sigmoid'))\\n\\nfc2 = Dense(3)                             #Output layer, don't need to mention input- it will take that from previous layer automatically\\nmodel.add(fc2)\\nmodel.add(Activation('softmax'))\\n\\nmodel.summary()\\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\"], ['import numpy as np\\nfrom matplotlib import pyplot as plt\\n%matplotlib inline\\n\\n#----------------keras imports-------------------#\\nimport keras\\nfrom keras.layers import Input, Dense, Activation\\nfrom keras.models import Sequential\\nfrom keras.utils import np_utils', 'mean_01 = np.array([3.0, 3.0])\\ncov_01 = np.array([[1.0, 0.0], [0.0, 1.0]])\\n\\nmean_02 = np.array([-3.0, 1.0])\\ncov_02 = np.array([[0.5, 0.4], [0.4, 0.7]])\\n\\nmean_03 = np.array([1.0, 1.0])\\ncov_03 = np.array([[0.5, 0.4], [0.4, 0.7]])\\n\\ndist_01 = np.random.multivariate_normal(mean_01, cov_01, 250)\\ndist_02 = np.random.multivariate_normal(mean_02, cov_02, 250)\\ndist_03 = np.random.multivariate_normal(mean_03, cov_03, 250)\\n\\ntotal_data_size = dist_01.shape[0] + dist_02.shape[0] + dist_03.shape[0]\\ndata_dim = dist_01.shape[1]\\n\\nlabels = np.zeros((total_data_size,))\\nlabels[dist_01.shape[0]:dist_01.shape[0]+dist_02.shape[0]] = 1\\nlabels[dist_01.shape[0]+dist_02.shape[0]:] = 2\\n\\ndata = np.zeros((total_data_size, data_dim + 1))\\n\\ndata[:dist_01.shape[0], :data_dim] = dist_01\\ndata[dist_01.shape[0]:dist_01.shape[0] + dist_02.shape[0], :data_dim] = dist_02\\ndata[dist_01.shape[0] + dist_02.shape[0]:, :data_dim] = dist_03\\n\\ndata[:, -1] = labels\\n\\nnp.random.shuffle(data)', \"plt.figure(0)\\n\\nfor ix in range(data.shape[0]):\\n    if data[ix, -1] == 0:\\n        plt.scatter(data[ix, 0], data[ix, 1], color='red')\\n    elif data[ix, -1] == 1:\\n        plt.scatter(data[ix, 0], data[ix, 1], color='green')\\n    else:\\n        plt.scatter(data[ix, 0], data[ix, 1], color='blue')\\nplt.show()\", 'split = int(total_data_size * 0.8)\\nX_train = data[:split, :-1]\\nX_val = data[split:, :-1]\\ny_train = np_utils.to_categorical(data[:split, -1])\\ny_val = np_utils.to_categorical(data[split:, -1])\\nprint X_train.shape, X_val.shape\\nprint y_train.shape, y_val.shape', \"# Keras model\\n\\nmodel = Sequential()\\n\\nfc1 = Dense(4, input_shape=(2,))           #This is the first hidden layer, therefore sigmoid function as activation and 4 neurons\\nmodel.add(fc1)\\nmodel.add(Activation('sigmoid'))\\n\\nfc2 = Dense(3)                             #Output layer, don't need to mention input- it will take that from previous layer automatically\\nmodel.add(fc2)\\nmodel.add(Activation('softmax'))\\n\\nmodel.summary()\\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\", 'hist = model.fit(X_train, y_train, nb_epoch = 200, shuffle=True, validation_data = (X_val, y_val))', 'print hist.history\\nprint hist.history.keys()', \"plt.figure(0)\\nplt.plot(hist.history['loss'], 'b')\\nplt.plot(hist.history['val_loss'], 'r')\\n\\nplt.figure(1)\\nplt.plot(hist.history['acc'], 'b')\\nplt.plot(hist.history['val_acc'], 'r')\\nplt.show()\", ''])\n"
     ]
    }
   ],
   "source": [
    "# extract markdown, language and comments\n",
    "# path = \"./sample_data/data/notebooks/\"\n",
    "# notebook_loc = path + 'nb_1222.ipynb'\n",
    "def get_text(path):\n",
    "    markdown = []\n",
    "    comments = []\n",
    "    code = []\n",
    "    with open(path) as fp:\n",
    "        try:\n",
    "            data = nbformat.read(fp, nbformat.NO_CONVERT)\n",
    "            #data = json.load(fp)\n",
    "        except:\n",
    "            return None, None, None\n",
    "    if 'cells' in data:\n",
    "        cells = data['cells']\n",
    "        if 'metadata' in data:\n",
    "            if 'kernelspec' in data['metadata']:\n",
    "                if 'language' in data['metadata']['kernelspec']:\n",
    "                    language = data['metadata']['kernelspec']['language']\n",
    "                else:\n",
    "                    language = None\n",
    "            else:\n",
    "                language = None\n",
    "        else:\n",
    "            language = None\n",
    "        md_cells = [c for c in cells if c['cell_type'] == 'markdown']\n",
    "        code_cells = [c for c in cells if c['cell_type'] == 'code']\n",
    "        for cell in md_cells:\n",
    "            markdown.append(cell['source'])\n",
    "\n",
    "        # find comments '# ' for R and Python\n",
    "        for cell in code_cells:\n",
    "            source = cell['source']\n",
    "            code.append(source)\n",
    "            string = ''\n",
    "            if source != None:\n",
    "                for item in source:\n",
    "                    string = string + str(item)\n",
    "                if '# ' in string:\n",
    "                    comments.append(string)\n",
    "        return language, markdown, comments, code\n",
    "    else:\n",
    "        return None, None, None, None\n",
    "    \n",
    "# zip version\n",
    "def get_text_zip(nb_id, nbzip):\n",
    "    markdown = []\n",
    "    comments = []\n",
    "    code = []\n",
    "    # raw = nbzip.read('nb_' + str(nb_id) + '.ipynb')\n",
    "    with nbzip.open('nb_' + str(nb_id) + '.ipynb') as raw:\n",
    "        try:\n",
    "            data = nbformat.read(raw, nbformat.NO_CONVERT)\n",
    "            #data = json.load(fp)\n",
    "        except:\n",
    "            return None, None, None, None\n",
    "    if 'cells' in data:\n",
    "        cells = data['cells']\n",
    "        if 'metadata' in data:\n",
    "            if 'kernelspec' in data['metadata']:\n",
    "                if 'language' in data['metadata']['kernelspec']:\n",
    "                    language = data['metadata']['kernelspec']['language']\n",
    "                else:\n",
    "                    language = None\n",
    "            else:\n",
    "                language = None\n",
    "        else:\n",
    "            language = None\n",
    "        md_cells = [c for c in cells if c['cell_type'] == 'markdown']\n",
    "        code_cells = [c for c in cells if c['cell_type'] == 'code']\n",
    "        for cell in md_cells:\n",
    "            markdown.append(cell['source'])\n",
    "\n",
    "        # find comments '# ' for R and Python\n",
    "        for cell in code_cells:\n",
    "            source = cell['source']\n",
    "            code.append(source)\n",
    "            string = ''\n",
    "            if source != None:\n",
    "                for item in source:\n",
    "                    string = string + str(item)\n",
    "                if '# ' in string:\n",
    "                    comments.append(string)\n",
    "        return language, markdown, comments, code\n",
    "    else:\n",
    "        return None, None, None, None\n",
    "\n",
    "# l,m,c = get_text(notebook_loc)\n",
    "#print(c)\n",
    "# print(get_text('bb2733859v_2_1/nb_0.ipynb'))\n",
    "print(get_text_zip(100002, nbzip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "O:\\Anaconda3\\lib\\site-packages\\nbformat\\validator.py:251: UserWarning: No schema for validating v2 notebooks\n",
      "  warnings.warn(\"No schema for validating v%s notebooks\" % version, UserWarning)\n",
      "O:\\Anaconda3\\lib\\site-packages\\nbformat\\validator.py:251: UserWarning: No schema for validating v1 notebooks\n",
      "  warnings.warn(\"No schema for validating v%s notebooks\" % version, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_id</th>\n",
       "      <th>html_url</th>\n",
       "      <th>name</th>\n",
       "      <th>language</th>\n",
       "      <th>markdown</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>https://github.com/crackcoder1291/SampleCard/b...</td>\n",
       "      <td>MeanMedianExercise.ipynb</td>\n",
       "      <td>python</td>\n",
       "      <td>[# Exercise: Mean &amp; Median Customer Spend, Her...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>https://github.com/Yanhao29/UW_MachineLearning...</td>\n",
       "      <td>week-5-lasso-assignment-1-blank.ipynb</td>\n",
       "      <td>python</td>\n",
       "      <td>[# Regression Week 5: Feature Selection and LA...</td>\n",
       "      <td>[from math import log, sqrt\\nsales['sqft_livin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>https://github.com/navjot12/Mini_ML_Projects/b...</td>\n",
       "      <td>Neural Network with 3 layers-checkpoint.ipynb</td>\n",
       "      <td>python</td>\n",
       "      <td>[### Therefore Training accuracy of 93.50% and...</td>\n",
       "      <td>[# Keras model\\n\\nmodel = Sequential()\\n\\nfc1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>https://github.com/navjot12/Mini_ML_Projects/b...</td>\n",
       "      <td>Neural Network with 3 layers.ipynb</td>\n",
       "      <td>python</td>\n",
       "      <td>[### Therefore Training accuracy of 93.50% and...</td>\n",
       "      <td>[# Keras model\\n\\nmodel = Sequential()\\n\\nfc1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>https://github.com/yama1968/Spikes/blob/46e450...</td>\n",
       "      <td>my_bayesian_survival_large-checkpoint.ipynb</td>\n",
       "      <td>python</td>\n",
       "      <td>[]</td>\n",
       "      <td>[# new model\\n\\ncensor = (lifetime + birth) &gt; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nb_id                                           html_url  \\\n",
       "0    1000  https://github.com/crackcoder1291/SampleCard/b...   \n",
       "1  100001  https://github.com/Yanhao29/UW_MachineLearning...   \n",
       "2  100002  https://github.com/navjot12/Mini_ML_Projects/b...   \n",
       "3  100003  https://github.com/navjot12/Mini_ML_Projects/b...   \n",
       "4  100004  https://github.com/yama1968/Spikes/blob/46e450...   \n",
       "\n",
       "                                            name language  \\\n",
       "0                       MeanMedianExercise.ipynb   python   \n",
       "1          week-5-lasso-assignment-1-blank.ipynb   python   \n",
       "2  Neural Network with 3 layers-checkpoint.ipynb   python   \n",
       "3             Neural Network with 3 layers.ipynb   python   \n",
       "4    my_bayesian_survival_large-checkpoint.ipynb   python   \n",
       "\n",
       "                                            markdown  \\\n",
       "0  [# Exercise: Mean & Median Customer Spend, Her...   \n",
       "1  [# Regression Week 5: Feature Selection and LA...   \n",
       "2  [### Therefore Training accuracy of 93.50% and...   \n",
       "3  [### Therefore Training accuracy of 93.50% and...   \n",
       "4                                                 []   \n",
       "\n",
       "                                            comments  \n",
       "0                                                 []  \n",
       "1  [from math import log, sqrt\\nsales['sqft_livin...  \n",
       "2  [# Keras model\\n\\nmodel = Sequential()\\n\\nfc1 ...  \n",
       "3  [# Keras model\\n\\nmodel = Sequential()\\n\\nfc1 ...  \n",
       "4  [# new model\\n\\ncensor = (lifetime + birth) > ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WITHOUT ZIP\n",
    "# append all extra columns to dataframe and export to csv\n",
    "begintime = time.time()\n",
    "final_df = pd.DataFrame(columns = COLUMNS)\n",
    "for i in range(len(ids)):\n",
    "    path = folder + '/nb_' + str(ids[i]) + '.ipynb'\n",
    "    language, markdown, comments = get_text(path)\n",
    "    if language != None or markdown != None or comments != None:\n",
    "        row = df_nb.loc[df_nb['nb_id'] == ids[i]]\n",
    "        final_df = final_df.append({COLUMNS[0]:row['nb_id'].values[0],\n",
    "                                    COLUMNS[1]:row['html_url'].values[0],\n",
    "                                    COLUMNS[2]:row['name'].values[0],\n",
    "                                    COLUMNS[3]:language,\n",
    "                                    COLUMNS[4]:markdown,\n",
    "                                    COLUMNS[5]:comments,\n",
    "                                    COLUMNS[6]:code},\n",
    "                                   ignore_index=True)\n",
    "# save the dataframe to csv\n",
    "midtime = time.time()\n",
    "print(midtime - starttime)\n",
    "final_df.to_csv('df_bb2733859v_2_1.csv')\n",
    "endtime = time.time()\n",
    "print(endtime - starttime)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "O:\\Anaconda3\\lib\\site-packages\\nbformat\\validator.py:251: UserWarning: No schema for validating v2 notebooks\n",
      "  warnings.warn(\"No schema for validating v%s notebooks\" % version, UserWarning)\n",
      "O:\\Anaconda3\\lib\\site-packages\\nbformat\\validator.py:251: UserWarning: No schema for validating v1 notebooks\n",
      "  warnings.warn(\"No schema for validating v%s notebooks\" % version, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16182.617602348328\n",
      "16209.22017788887\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_id</th>\n",
       "      <th>html_url</th>\n",
       "      <th>name</th>\n",
       "      <th>language</th>\n",
       "      <th>markdown</th>\n",
       "      <th>comments</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120951</td>\n",
       "      <td>https://github.com/tuckercs/DSI-Projects/blob/...</td>\n",
       "      <td>Fizz Buzz.ipynb</td>\n",
       "      <td>python</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[import pandas as pd, def fizzbuzz(n):\\n    if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23352</td>\n",
       "      <td>https://github.com/Isabella-Dineo/beam_py/blob...</td>\n",
       "      <td>patchbeam_main.ipynb</td>\n",
       "      <td>python</td>\n",
       "      <td>[]</td>\n",
       "      <td>[def correct(x):\\n    \"\"\"Function to correct f...</td>\n",
       "      <td>[import numpy as np\\nimport scipy as sp\\nfrom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73638</td>\n",
       "      <td>https://github.com/wanglongjuan/15-ODE-homewor...</td>\n",
       "      <td>2015750221张杰-checkpoint.ipynb</td>\n",
       "      <td></td>\n",
       "      <td>[数学与应用数学班, 张杰 2015750221, **P88——3**, **求初值问题$...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>179265</td>\n",
       "      <td>https://github.com/tyui592/nn_study/blob/f3c37...</td>\n",
       "      <td>hand_write_function_learning.ipynb</td>\n",
       "      <td>python</td>\n",
       "      <td>[# Artificial Neural Network, ## Object : hand...</td>\n",
       "      <td>[# read hand write function image\\nimg = color...</td>\n",
       "      <td>[from skimage import io, color\\nimport matplot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54493</td>\n",
       "      <td>https://github.com/suhagba/Naive_Bayes/blob/e6...</td>\n",
       "      <td>Tutorial5.ipynb</td>\n",
       "      <td>python</td>\n",
       "      <td>[**Name:** Byaravalli Arun Suhag\\n\\n**EID:** 5...</td>\n",
       "      <td>[%matplotlib inline\\nimport IPython.core.displ...</td>\n",
       "      <td>[%matplotlib inline\\nimport IPython.core.displ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nb_id                                           html_url  \\\n",
       "0  120951  https://github.com/tuckercs/DSI-Projects/blob/...   \n",
       "1   23352  https://github.com/Isabella-Dineo/beam_py/blob...   \n",
       "2   73638  https://github.com/wanglongjuan/15-ODE-homewor...   \n",
       "3  179265  https://github.com/tyui592/nn_study/blob/f3c37...   \n",
       "4   54493  https://github.com/suhagba/Naive_Bayes/blob/e6...   \n",
       "\n",
       "                                 name language  \\\n",
       "0                     Fizz Buzz.ipynb   python   \n",
       "1                patchbeam_main.ipynb   python   \n",
       "2       2015750221张杰-checkpoint.ipynb            \n",
       "3  hand_write_function_learning.ipynb   python   \n",
       "4                     Tutorial5.ipynb   python   \n",
       "\n",
       "                                            markdown  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [数学与应用数学班, 张杰 2015750221, **P88——3**, **求初值问题$...   \n",
       "3  [# Artificial Neural Network, ## Object : hand...   \n",
       "4  [**Name:** Byaravalli Arun Suhag\\n\\n**EID:** 5...   \n",
       "\n",
       "                                            comments  \\\n",
       "0                                                 []   \n",
       "1  [def correct(x):\\n    \"\"\"Function to correct f...   \n",
       "2                                                 []   \n",
       "3  [# read hand write function image\\nimg = color...   \n",
       "4  [%matplotlib inline\\nimport IPython.core.displ...   \n",
       "\n",
       "                                                code  \n",
       "0  [import pandas as pd, def fizzbuzz(n):\\n    if...  \n",
       "1  [import numpy as np\\nimport scipy as sp\\nfrom ...  \n",
       "2                                                 []  \n",
       "3  [from skimage import io, color\\nimport matplot...  \n",
       "4  [%matplotlib inline\\nimport IPython.core.displ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append all extra columns to dataframe and export to csv, zip \n",
    "starttime = time.time()\n",
    "final_df = pd.DataFrame(columns = COLUMNS)\n",
    "for i in range(len(ids)):\n",
    "    language, markdown, comments, code = get_text_zip(ids[i], nbzip)\n",
    "    if language != None or markdown != None or comments != None or code != None:\n",
    "        row = df_nb.loc[df_nb['nb_id'] == ids[i]]\n",
    "        final_df = final_df.append({COLUMNS[0]:row['nb_id'].values[0],\n",
    "                                    COLUMNS[1]:row['html_url'].values[0],\n",
    "                                    COLUMNS[2]:row['name'].values[0],\n",
    "                                    COLUMNS[3]:language,\n",
    "                                    COLUMNS[4]:markdown,\n",
    "                                    COLUMNS[5]:comments,\n",
    "                                    COLUMNS[6]:code},\n",
    "                                   ignore_index=True)\n",
    "# save the dataframe to pickle\n",
    "midtime = time.time()\n",
    "print(midtime - starttime)\n",
    "final_df.to_pickle('df_bb2733859v_2_1.pkl')\n",
    "endtime = time.time()\n",
    "print(endtime - starttime)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_df = pd.read_csv('df_bb2733859v_2_1.csv')\n",
    "\n",
    "bulk_data = []\n",
    "\n",
    "for index, row in es_df.iterrows():\n",
    "    data_dict = {}\n",
    "    for i in range(len(row)):\n",
    "        data_dict[es_df.columns[i]] = row[i]\n",
    "    op_dict = {\n",
    "        \"index\": {\n",
    "            \"_index\": INDEX,\n",
    "            \"_type\": TYPE,\n",
    "            \"_id\": es_df['nb_id']\n",
    "        }\n",
    "    }\n",
    "    bulk_data.append(op_dict)\n",
    "    bulk_data.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'index': 'notebookindex', 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise the elastic search index\n",
    "init_index = {\n",
    "    \"settings\" : {\n",
    "        \"number_of_shards\": 5,\n",
    "        \"number_of_replicas\": 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"text_analyzer\": {\n",
    "                    \"type\": \"standard\",\n",
    "                    \"stopwords\": \"_english_\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'nb_id': {'type': 'integer'},\n",
    "                'html_url': {'type': 'keyword'},\n",
    "                'name': {'type': 'keyword'},\n",
    "                'language': {'type': 'keyword'},\n",
    "                'markdown': {'type': 'text', \n",
    "                             \"analyzer\":\"text_analyzer\"\n",
    "                            },\n",
    "                'comments': {'type': 'text', \n",
    "                             \"analyzer\":\"text_analyzer\"\n",
    "                            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.indices.create(index = INDEX, body = init_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'notebookindex': {'mappings': {'properties': {'comments': {'type': 'text'},\n",
       "    'html_url': {'type': 'keyword'},\n",
       "    'language': {'type': 'keyword'},\n",
       "    'markdown': {'type': 'text'},\n",
       "    'name': {'type': 'keyword'},\n",
       "    'nb_id': {'type': 'integer'}}}}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the data and structure is in elasticsearch\n",
    "# es.search(body={\"query\": {\"match_all\": {}}}, index = INDEX)\n",
    "es.indices.get_mapping(index = INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
